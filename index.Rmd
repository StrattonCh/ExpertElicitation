---
title: "Statistical methods for incoporating expert elicitation in occupancy modeling frameworks"
description: |
  A simulation study in implementation of hierarchical models to accommodate expert elicitation with bat occupancy data
author:
  - name: Christian Stratton
    affiliation: Montana State University
    affiliation_url: https://math.montana.edu/
  - name: Kathryn Irvine
    affiliation: USGS Northern Rocky Mountain Science Center
    affiliation_url: https://www.usgs.gov/centers/norock
  - name: Brad Udell
    affiliation: USGS Fort Collins Science Center
    affiliation_url: https://www.usgs.gov/centers/fort-collins-science-center
  - name: Brian Reichert
    affiliation: USGS Fort Collins Science Center
    affiliation_url: https://www.usgs.gov/centers/fort-collins-science-center
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
bibliography: bibliography.bib
csl: biometrics_notes.csl
---

```{r setup, include = F}
rm(list = ls())

library(knitr)
hook_chunk <- knitr::knit_hooks$get('chunk')
knit_hooks$set(chunk = function(x, options) {

  # add latex commands if chunk option singlespacing is TRUE
  if(isTRUE(options$singlespacing)){
    return(sprintf("\\singlespacing\n %s \n\\doublespacing", hook_chunk(x, options)))
  } else{
    return(hook_chunk(x, options))
  }
})
knitr::opts_chunk$set(
  fig.align = "center",
  tidy = T,
  singlespacing = TRUE,
  cache = FALSE,
  fig.dim = c(10,8),
  message = FALSE,
  warning = FALSE,
  comment = NA,
  echo = F
)


# packages
packs <- c("dplyr", "htmltools", "ggplot2", "sf", "readr", "parallel", "xtable", "tidyr", "stringr", "ggalluvial", "lubridate", "coda", "rstan",  "tidyverse")
sapply(packs, require, character.only = T)
rm(packs)
options(tidyverse.quiet = TRUE)

# convenience
`%notin%` <- Negate("%in%")

# stan settings
options(mc.cores = parallel::detectCores() - 1)
rstan_options(auto_write = TRUE)

# useful fuctions
nimble_summary <- function(fit, warmup = nrow(fit[[1]])/2, thin = 1){
  # convert to coda for normal summary
  fit_warmup <- lapply(fit, function(x) x[(warmup+1):nrow(x),])
  coda_samples <- as.mcmc.list(lapply(fit_warmup, function(x) as.mcmc(
    x, start = warmup+1, end = nrow(fit), thin = thin
  )))
  
  sum <- summary(coda_samples)
  params <- dimnames(sum$statistics)[[1]]
  tmp_sum <- cbind(sum$statistics, sum$quantiles)
  
  # get r hat / n_eff
  mat <- matrix(NA, nrow = nrow(tmp_sum), ncol = 3)
  colnames(mat) <- c("Rhat", "ess_bulk", "ess_tail")
  for(i in 1:nrow(tmp_sum)){
    tmp <- sapply(fit, function(x) x[,i])
    mat[i,] <- c(Rhat(tmp), ess_bulk(tmp), ess_tail(tmp))
  }
  
  # out 
  out <- cbind(tmp_sum, mat)
  return(out)
}
trace_plot <- function(samples, which = c("sigma")){
  niter <- dim(samples[[1]])[1]
  nchains <- length(samples)
  params <- colnames(samples[[1]])
  
  combined_samples <- do.call("rbind", samples)
  combined_samples <- combined_samples %>%
    as_tibble %>%
    mutate(iter = rep(1:niter, nchains), chain = factor(rep(1:nchains, each = niter))) %>%
    dplyr::select(chain, iter, everything()) %>%
    pivot_longer(cols = -c(1:2), names_to = "param", values_to = "trace")
  
  if(all(is.numeric(which))) which <- unique(combined_samples$param)[which]
  
  p <- combined_samples %>%
    filter(param %in% which) %>%
    ggplot() + 
    geom_line(aes(x = iter, y = trace, col = chain, group = chain)) +
    theme_bw() +
    facet_wrap(~ param, scales = "free_y")
  
  return(p)
  
}
readJMCMC <- function(dir){
  tmp <- readRDS(dir)
  
  out <- list()
  for(i in 1:length(tmp)){
    tmp2 <- tmp[[i]][["samples"]]
    colnames(tmp2) <- tmp[[i]][["names"]]
    out[[i]] <- tmp2
  }
  
  return(out)
}
```

# Introduction

# Model framework

## Base site-occupancy modeling framework

For now, we layer this framework on a standard occupancy model. Let $i$ index the location, $j$ index the visit to site $i$, and $Z_i$ denote the partially observed occupancy states of sites $1, \dots, n$. Then,
\[
\begin{split}
Z_i &\sim \text{Bernoulli}(\psi_i) \\
y_i &\sim \text{Bernoulli}(z_i p_{ij})
\end{split}
\]
where $\text{logit}(\psi_i) = x_i'\beta$ and $\text{logit}(p_{ij}) = W\alpha$.

## Addition of random effects

To accommodate extra sources of variation not captured by $X$, we include spatially indexed random effects for each site. 
\[
\begin{split}
Z_i &\sim \text{Bernoulli}(\psi_i) \\
y_i &\sim \text{Bernoulli}(z_i p_{ij})
\end{split}
\]
where $\text{logit}(\psi_i) = x_i'\beta + \theta_{i}$ and $\text{logit}(p_{ij}) = W\alpha$.

Additional structure may be imposed on $\boldsymbol{\theta}$. For example, one could draw $\boldsymbol{\theta}$ from a Gaussian process with spatial covariance structure. Alternatively, $\boldsymbol{\theta}$ may be drawn from a hierarchical normal distribution if expert information is available to inform a grouping structure among sites.
\[
\boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\tau} \boldsymbol{\Omega}\boldsymbol{\tau}^T) 
\]
Under this formulation, $\text{logit}(\psi_i) = x_i'\beta + \theta_{c_i}$, where $\boldsymbol{c}$ is a vector of indicators denoting the group associated with $\psi_i$. Note that the above formulation assume a general correlation structure among $\boldsymbol{\theta}$ (in the form of $\boldsymbol{\Omega}$), but independence may be assumed. Under this framework, expert knowledge may be used to inform $\boldsymbol{\mu}$. For example, 
\[
\boldsymbol{\mu} \sim \mathcal{N}(\boldsymbol{m}, \boldsymbol{s}\mathcal{I}\boldsymbol{s}^T)
\]

# Expert response structure

The goal is to collect information from experts in a way that can easily inform the prior distributions for the spatial random effects. One option is to solicit feedback directly on the occupancy probabilities, then map those probabilities to the real line. 

## Daubenmire classes

To aid in describing the uncertainties in this mapping, one could use the Daubenmire coverage class [@daubenmire1959]. 

```{r, results = "asis"}
logit <- function(p) log(p / (1-p))
daub <- round(tibble(
  `Class` = 1:6,
  `Lower` = c(0, .05, .25, .50, .75, .95),
  `Upper` = c(.05, .25, .50, .75, .95, 1)
) %>%
  mutate(`Midpoint` = (Lower + Upper)/2) %>%
  mutate(
    logitL = logit(Lower+.01), 
    logitM = logit(Midpoint),
    logitU = logit(Upper-.01)
  ), 2)

knitr::kable(daub, caption = "Table 1: Daubenmire coverage classes (Daubenmire, 1959).")
```

## Normal k-root-mean-weighted mapping

There are many potential mappings for expert feedback in the form of Daubenmire classes. Generally, the mapping should imply lesser probability of occurrence for lower classes. Additionally, it may be reasonable to imply greater certainty in large classes (i.e. probabilities close to 0 and 1 are more certain than those close to 0). Finally, it is computationally attractive to restrict mappings to the Normal family of distributions (though this can be relaxed). A mapping that satisfies these requirements is described below.

Let $l_i, u_i$, and $\mu_i$ denote the implied lower, upper, and midpoints of each class on the logit scale. Then the normal mean-weighted mapping proceeds as follows. For each class $i$ and any positive real $k$, find $\sigma_i$ such that 
\[
\int_{l_i}^{u_i} p\left(y \big| \mu_i,\frac{1}{|\mu_i|^k} \sigma_i\right) dy = .95
\]
where $p(y | \mu, \sigma)$ is the density function for a normal random variate with mean $\mu$ and standard deviation $\sigma$. 

```{r, eval = T, warning = F}
# function to fit normal distribution by quantiles on logit scale
fitnorm <- function(lwr, upr, delta = 1e-3, root = 2){
  # fit normal distribution such that
  ## mu = logit(midpt)
  ## Pr(Y < logit(lwr)) = .025
  logit <- function(p) log(p / (1-p))

  if(lwr == 0) lwr <- lwr + delta
  if(upr == 1) upr <- upr - delta
  
  # precalc quantities
  midpt <- (lwr + upr)/2
  mu <- logit(midpt)
  
  f <- function(gamma){
    pnorm(logit(upr), mu, abs(mu)^(1/root) * exp(gamma)) - pnorm(logit(lwr), mu, abs(mu)^(1/root) * exp(gamma)) - .95
  }
  root <- uniroot(f, c(-10, 10))
  return(c(mu, exp(root$root)))
}

# plot it
plot_mapping <- function(delta, k){
  if(length(k) == 1){
    # get probabilities
    p <- seq(0.001, .999, by = .001)
    logitp <- log(p / (1 - p)) 
    mat <- matrix(NA, length(logitp), nrow(daub))
    for(j in 1:ncol(mat)){
      params <- unname(fitnorm(unlist(daub[j,2]), unlist(daub[j,3]), delta = delta, root = k))
      mat[,j] <- dnorm(
        logitp, 
        params[1],
        params[2]
      )
    }
    
    # create plot
    tibble(
      logitp = logitp
    ) %>%
      bind_cols(., as_tibble(mat)) %>%
      rename(
        class1 = 2, class2 = 3, class3 = 4,
        class4 = 5, class5 = 6, class6 = 7
      ) %>%
      pivot_longer(
        class1:class6, 
        names_to = "class",
        values_to = "density"
      ) %>%
      mutate(
        density = ifelse(density < 1e-6, NA, density)
      ) %>%
      mutate(root = paste0("root = ", k)) %>%
      na.omit %>%
      ggplot() +
      geom_line(aes(x = logitp, y = density, group = class, col = class)) +
      theme_bw() +
      facet_wrap(~ root)
  } else{
    out <- list()
    for(iter in 1:length(k)){
      # get probabilities
      p <- seq(0.001, .999, by = .001)
      logitp <- log(p / (1 - p)) 
      mat <- matrix(NA, length(logitp), nrow(daub))
      for(j in 1:ncol(mat)){
        params <- unname(fitnorm(unlist(daub[j,2]), unlist(daub[j,3]), delta = delta, root = k[iter]))
        mat[,j] <- dnorm(
          logitp, 
          params[1],
          params[2]
        )
      }
      
      # create plot
      out[[iter]] <- tibble(
        logitp = logitp
      ) %>%
        bind_cols(., as_tibble(mat)) %>%
        rename(
          class1 = 2, class2 = 3, class3 = 4,
          class4 = 5, class5 = 6, class6 = 7
        ) %>%
        pivot_longer(
          class1:class6, 
          names_to = "class",
          values_to = "density"
        ) %>%
        mutate(
          density = ifelse(density < 1e-6, NA, density)
        ) %>%
        mutate(root = paste0("root = ", k[iter])) %>%
        na.omit
    }
    
    do.call("rbind", out) %>%
      ggplot() +
      geom_line(
        aes(
          x = logitp, y = density, 
          group = class, col = class
        )
      ) +
      theme_bw() +
      facet_wrap(~ root, nrow = length(k))
  }
}
plot_mapping(1e-3, c(2,3))
```

# Model estimation

To improve computational efficiency, we write the sampler for this model by hand. Leveraging the data augmentation strategy of @polson2013, Gibbs draws are available for all parameters in the model. The full specification of the model is as follows. Let $i$ index the sample location, $j$ index the visit to site $i$, and $\boldsymbol{c}$ denote a vector group membership informed by expert opinion. Then,
\[
\begin{split}
Z_i &\sim \text{Bernoulli}(\psi_i) \\
y_{ij} &\sim \text{Bernoulli}(z_i p_{ij})
\end{split}
\]
where $\text{logit}(\psi_i) = \boldsymbol{x}_i^T\boldsymbol{\beta}+ \theta_{c_i}$ and
\[
\boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\mu}, \tau^2 \boldsymbol{\mathcal{I}})
\]
For now, we assume that $p_{ij} = p$ for all $\{i, j\}$ to speed up computation (though this assumption can be easily relaxed). 

## Polya-gamma data augmentation

For binomial and negative binomial sampling models, we implement the Polya-gamma data augmentation strategy of @polson2013. This strategy introduces auxiliary Polya-gamma distributed random variables to each likelihood to afford conditionally Gaussian posterior distributions, allowing for Gibbs draws of regression coefficients. This data augmentation is relevant for sampling $\alpha, \beta$, and $\eta$. 

## Sampling $\boldsymbol{z}^{(\psi)}$

We let $\boldsymbol{z}^{(\psi)}$ denote the vector of partially observed occupancy states and $y_{ij}$ denote the observed response during visit $j$ to site $i$. From @dorazio2018, the full conditional distribution of $\boldsymbol{z}^{(\psi)}$ is,
\[
Z_i^{(\psi)} \sim 
\begin{cases}
\text{Bernoulli}(1) & \text{if} \sum_{j=1}^{J_i} y_{ij} > 0 \\
\text{Bernoulli}\left(\frac{\psi_i\prod_{j=1}^{J_i} (1 - p_{ij})}{1 -\psi_i + \psi_i\prod_{j=1}^{J_i} (1 - p_{ij})}\right) & \text{if} \sum_{j=1}^{J_i} y_{ij} = 0
\end{cases}
\]

## Sampling $\boldsymbol{\omega}^{(\beta)}$

Let $\omega^{(\beta)}_i \sim \text{PG}(1, 0)$. Then,
\[
z_i = \frac{1}{\omega^{(\beta)}_i}\left(z^{(\psi)}_i - \frac{1}{2}\right) \sim \mathcal{N}\left(\boldsymbol{x}_i'\boldsymbol{\beta} + \theta_{c_i}, \frac{1}{\omega_i} \right).
\]
where $z^{(\psi)}_i$ is the latent occupancy state for site $i$. See @polson2013 for more detail. Equivalently, let $\boldsymbol{\Omega}_{(\beta)} = \text{diag}(\omega^{(\beta)}_1, \dots, \omega^{(\beta)}_n)$. Then,
\[
\boldsymbol{z} \sim \mathcal{N}\left(X\boldsymbol{\beta}+ \boldsymbol{C}\boldsymbol{\theta}, \boldsymbol{\Omega}_{(\beta)}^{-1}\right)
\]
where $\boldsymbol{C}$ is an indicator matrix denoting group membership. From @polson2013, the full conditional distribution of $\omega^{(\beta)}_i$ is
\[
\begin{split}
\omega^{(\beta)}_i | \cdot \sim \text{PG}(1, \boldsymbol{x}_i'\boldsymbol{\beta} + \theta_{c_i})
\end{split}
\]

## Sampling $\boldsymbol{\beta}$

Let $\boldsymbol{\beta} \sim \mathcal{N}(\boldsymbol{\mu}_{0, \beta}, \boldsymbol{\Sigma}_{0, \beta})$.Then,
\[
\begin{split}
\boldsymbol{\beta} | \cdot &\sim \mathcal{N}(\boldsymbol{m}, \boldsymbol{V}) \\
\boldsymbol{V} &= \left(\boldsymbol{X}^T \boldsymbol{\Omega}_{(\beta)} \boldsymbol{X} +  \boldsymbol{\Sigma}^{-1}_{0, \beta}\right)^{-1} \\
\boldsymbol{m} &= \boldsymbol{V} \left(\boldsymbol{X}^T \boldsymbol{\Omega}_{(\beta)} (\boldsymbol{z} - \boldsymbol{C}\boldsymbol{\theta}) + \boldsymbol{\Sigma}^{-1}_{0, \beta}\boldsymbol{\mu}_{0, \beta} \right)
\end{split}
\]

Proof (omitting boldface text):

Let $z^* = z - C\theta$
\[
\begin{split}
p(\beta | z^*, \omega^{(\beta)}) &\propto p(z^* | \beta, \omega^{(\beta)}) p(\beta) \\
&\propto \exp\left\{-\frac{1}{2}(z^* - X\beta)^T \Omega_{(\beta)}(z^* - X\beta) \right\} \exp\left\{-\frac{1}{2}(\beta - \mu_{0, \beta})^T \Sigma^{-1}_{(0,\beta)}(\beta - \mu_{0, \beta}) \right\}  \\
&\propto \exp \left\{-\frac{1}{2} \left(\beta^T (X^T \Omega_{(\beta)} X + \Sigma^{-1}_{0, \beta})  \beta - 2\beta^T( X^T \Omega_{(\beta)} z^* + \Sigma^{-1}_{0, \beta}\mu_{0, \beta} )\right) \right\}
\end{split}
\]

## Sampling $\boldsymbol{\theta}$

Let $\boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\mu}, \tau^2\boldsymbol{\mathcal{I}})$. Note that the expert elicitation informs $\boldsymbol{\mu}$. The full conditional posterior distribution of $\boldsymbol{\theta}$ is
\[
\begin{split}
\boldsymbol{\theta} | \cdot &\sim \mathcal{N}(\boldsymbol{m}, \boldsymbol{V}) \\
\boldsymbol{V} &= \left(\boldsymbol{C}^T \boldsymbol{\Omega}_{(\beta)} \boldsymbol{C} +  \frac{1}{\tau^2}\boldsymbol{\mathcal{I}}\right)^{-1} \\
\boldsymbol{m} &= \boldsymbol{V} \left(\boldsymbol{C}^T \boldsymbol{\Omega}_{(\beta)} (\boldsymbol{z} - \boldsymbol{X}\boldsymbol{\beta}) + \frac{1}{\tau^2} \boldsymbol{\mathcal{I}}\boldsymbol{\mu} \right)
\end{split}
\]

Proof (omitting boldface text):

Let $z^* = z - X\beta$
\[
\begin{split}
p(\beta | z^*, \omega^{(\beta)}) &\propto p(z^* | \theta, \omega^{(\beta)}) p(\theta) \\
&\propto \exp\left\{-\frac{1}{2}(z^* - C\theta)^T \Omega_{(\beta)}(z^* - C\theta) \right\} \exp\left\{-\frac{1}{2}(\theta - \mu)^T \frac{1}{\tau^2}\mathcal{I}(\theta - \mu) \right\}  \\
&\propto \exp \left\{-\frac{1}{2} \left(\theta^T (C^T \Omega_{(\beta)} C + \frac{1}{\tau^2}\mathcal{I})  \theta - 2\theta^T( C^T \Omega_{(\beta)} z^* + \frac{1}{\tau^2} \mathcal{I}\mu )\right) \right\}
\end{split}
\]

## Sampling $\boldsymbol{\mu}$

Let $\boldsymbol{\mu} \sim \mathcal{N}(\boldsymbol{m}, \boldsymbol{s}\mathcal{I}\boldsymbol{s}^T)$, where $\boldsymbol{m}$ and $\boldsymbol{s}$ may be informed by expert opinion. Then
\[
\begin{split}
\boldsymbol{\mu} | \cdot &\sim \mathcal{N}(\boldsymbol{m}_0, \boldsymbol{V})  \\
\boldsymbol{V} &= \left(\frac{1}{\tau^2} \boldsymbol{\mathcal{I}} + \boldsymbol{s}^{-1}\boldsymbol{\mathcal{I}} (\boldsymbol{s}^{-1})^T \right)^{-1} \\
\boldsymbol{m}_0 &= \boldsymbol{V}\left(\frac{1}{\tau^2} \boldsymbol{\mathcal{I}}\boldsymbol{\theta} + \boldsymbol{s}^{-1}\boldsymbol{\mathcal{I}} (\boldsymbol{s}^{-1})^T \boldsymbol{m} \right) 
\end{split}
\]

Proof (omitting boldface text):
\[
\begin{split}
p(\mu | \cdot) &\propto p(\theta | \mu, \tau^2) p(\mu) \\
&\propto \exp\left\{-\frac{1}{2}(\theta - \mu)^T \frac{1}{\tau^2}\mathcal{I}(\theta - \mu) \right\} \exp\left\{-\frac{1}{2}(\mu - m)^T s^{-1} \mathcal{I} s^{-1} (\mu - m) \right\} \\
&\propto \exp\left\{ -\frac{1}{2} \left(\mu^T (\frac{1}{\tau^2} \mathcal{I} + s^{-1} \mathcal{I} s^{-1})\mu - 2\mu^T(\frac{1}{\tau^2}\mathcal{I}\theta + s^{-1} \mathcal{I} s^{-1} m) \right) \right\}
\end{split}
\]

## Sampling $\tau$

Let $\tau^2 \sim \text{Inverse-Gamma}(a_0, b_0)$. Then,
\[
\begin{split}
\tau^2 | \cdot &\sim \text{Inverse-Gamma}(a, b) \\
a &= a_0 + \frac{K}{2} \\
b &= b_0 + \frac{1}{2}(\boldsymbol{\theta} - \boldsymbol{\mu})^T(\boldsymbol{\theta} - \boldsymbol{\mu})
\end{split}
\]

Proof (omitting boldface):
\[
\begin{split}
p(\tau^2 | \theta, \mu) &\propto p(\theta | \tau^2, \mu) p(\tau^2) \\
&\propto (\tau^2)^{-K/2} \exp\left\{-\frac{1}{2}(\theta - \mu)^T \frac{1}{\tau^2} \mathcal{I}(\theta - \mu)\right\} (\tau^2)^{-a_0 - 1} \exp\left(-\frac{b_0}{\tau^2}\right)
\end{split}
\]


# Synthetic data scenarios

## Idyllic case

In the ideal case, species occupancy arises as a function of some linear relationship with covariates and a regional intercept that is not directly observed. The experts then inform this regional intercept. 

### Sampling model

We first imagine the data arising directly from the specified model. That is, 
\[
\begin{split}
Z_i &\sim \text{Bernoulli}(\psi_i) \\
y_i &\sim \text{Bernoulli}(z_i p_{ij})
\end{split}
\]
where $\text{logit}(\psi_i) = x_i'\beta + \theta_{c_i}$ and $\text{logit}(p_{ij}) = W\alpha$. Furthermore, we assume that $\theta_{c_i} \sim N(\mu_{c_i}, \tau^2)$, where $c_i$ is an indicator vector denoting distinct regions assigned by the expert. For now, we assume that $p$ is constant (to speed up simulated studies). 

### Simulated data

```{r}
# functions to simulate data
fitnorm <- function(lwr, upr, delta = 1e-3, root = 2){
  # fit normal distribution such that
  ## mu = logit(midpt)
  ## Pr(Y < logit(lwr)) = .025
  logit <- function(p) log(p / (1-p))
  
  if(lwr == 0) lwr <- lwr + delta
  if(upr == 1) upr <- upr - delta
  
  # precalc quantities
  midpt <- (lwr + upr)/2
  mu <- logit(midpt)
  
  f <- function(gamma){
    pnorm(logit(upr), mu, abs(mu)^(1/root) * exp(gamma)) - pnorm(logit(lwr), mu, abs(mu)^(1/root) * exp(gamma)) - .95
  }
  root <- uniroot(f, c(-10, 10))
  return(c(mu, exp(root$root)))
}
split_poly <- function(grid, n_areas){
  # combine
  sf_poly = grid %>% st_as_sf %>% st_union 
  
  # create random points
  points_rnd <- st_sample(sf_poly, size = 10000)
  
  # k-means clustering
  points <- do.call(rbind, st_geometry(points_rnd)) %>%
    as_tibble() %>%
    setNames(c("lon","lat"))
  k_means <- kmeans(points, centers = n_areas)
  
  # create voronoi polygons
  voronoi_polys <- st_voronoi(
    st_as_sf(
      k_means$centers %>% 
        as_tibble, 
      coords = c("lon", "lat")
    ) %>% st_union,
    sf_poly
  )
  
  # convert to multipolygons
  tmp <- st_cast(voronoi_polys) %>%
    st_as_sf()
  
  group <- matrix(NA, nrow(grid), 1)
  coverby <- st_covered_by(grid %>% st_as_sf, tmp %>% st_buffer(.5))
  for(i in 1:nrow(grid)){
    if(length(coverby[[i]]) == 0){
      NULL
    } else{
      group[i] <- coverby[[i]]  
    }
  }
  out <- grid
  out$group <- c(group)
  
  out %>%
    st_as_sf %>% 
    ggplot() + 
    geom_sf(aes(fill = group))
  return(out)
}
sim_idyllic <- function(n = 100, J = 8, seed = 1, beta = c(0,1), p = .5, tau = 1, nregions = 4, root = 3){
  # housekeeping
  if((sqrt(n) %% 1) != 0) stop("n should be a perfect square")
  set.seed(seed)
  sigma = 1
  phi = 3
  
  # bring along the daub classes
  logit <- function(p) log(p / (1-p))
  daub <- round(tibble(
    `Class` = 1:6,
    `Lower` = c(0, .05, .25, .50, .75, .95),
    `Upper` = c(.05, .25, .50, .75, .95, 1)
  ) %>%
    mutate(`Midpoint` = (Lower + Upper)/2) %>%
    mutate(
      logitL = logit(Lower+.01), 
      logitM = logit(Midpoint),
      logitU = logit(Upper-.01)
    ), 2)
  
  distr_tbl <- t(apply(
    daub %>% dplyr::select(Lower, Upper) %>% as.matrix,
    1, 
    function(x) fitnorm(x[1], x[2], delta = 1e-3, root = root)
  )) %>% unname
  distr_tbl <- rbind(
    distr_tbl,
    c(0, 1)
  )
  
  # create grid
  sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(sqrt(n),0), c(sqrt(n),sqrt(n)), c(0,0)))))
  grid <- st_as_sf(st_make_grid(sfc, cellsize = 1, square = TRUE)) %>% as_tibble
  names(grid) <- "geometry"
  rm(sfc)
  
  # add centroid and assign group
  grid <- split_poly(grid, nregions)
  
  # assign normal distributions to each region
  if(nregions != 6) stop("must have 6 regions for now")
  grid$group <- with(grid, ifelse(is.na(group), 7, group))
  
  # spatial random effects
  coords <- suppressWarnings({grid %>%
      st_as_sf %>%
      st_centroid %>%
      st_coordinates})
  dist_mat <- coords %>%
    as.matrix %>%
    dist %>% as.matrix
  
  # generate spatial covariate
  Sigma <- sigma^2 * exp(-dist_mat^2 / (2*phi^2)) + diag(1e-1, dim(dist_mat))
  grid$x <- c(mvtnorm::rmvnorm(1, rep(0, n), Sigma))
  
  # generate spatially indexed random effects
  mu <- apply(distr_tbl, 1, function(x) rnorm(1, x[1], x[2]))
  theta <- sapply(mu, function(x) rnorm(1, x, tau))
  grid$theta <- sapply(grid$group, function(x) theta[x])
  
  # generate z states
  grid$linpred <- beta[1] + beta[2]*grid$x + grid$theta
  grid$psi <- exp(grid$linpred) / (1 + exp(grid$linpred))
  grid$z <- rbinom(nrow(grid), 1, grid$psi)
  
  # generate observed detections
  grid$y <- rbinom(nrow(grid), J, grid$z * p)
  out <- list(
    df = grid,
    params = list(
      beta = beta, theta = c(theta), p = p, psi = grid$psi, z = grid$z, tau = tau, mu = mu
    ),
    expert_dists = distr_tbl
  )
  
  return(out)
}

# sim data
sim_dat <- sim_idyllic(n = 20^2, seed = 2, nregions = 6, beta = c(0, 1))

# plot
p3 <- sim_dat$df %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = psi)) +
  theme_bw()
p1 <- sim_dat$df %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = x)) +
  theme_bw()
p4 <- sim_dat$df %>%
  mutate(z = factor(z)) %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = z)) +
  theme_bw()
p2 <- sim_dat$df %>%
  mutate(z = factor(z)) %>%
  mutate(naive_p = y/8) %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = theta)) +
  theme_bw()

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

### Fit models with Julia

We consider two potential models:

  - model0: basic occupancy model without region-specific random effects
  - model1: occupancy model with region-specific random effects informed by experts

For the time being, we assume the experts are correct.

```{r, eval = F}
# export data list to Julia
data_list <- list(
  y = sim_dat$df$y,
  J = 8,
  X = cbind(rep(1, nrow(sim_dat$df)), sim_dat$df$x),
  C = model.matrix(~ factor(sim_dat$df$group,) - 1) %>% unname %>% as.matrix,
  expert_means = sim_dat$expert_dists[,1],
  expert_sds = sim_dat$expert_dists[,2]
)
saveRDS(data_list, "julia code/idyllic/data_list.rds")
```

#### model0

```{r, eval = T}
# load fitted model from Julia
fit_parallel <- readJMCMC("julia code/idyllic/model0.rds")
tmp <- nimble_summary(fit_parallel, warmup = 0)
tmp[1:11,c(1,5,9:11)] 

# prediction
tbl0 <- tibble(
  param = rownames(tmp),
  mean = tmp[,1],
  lwr = tmp[,5],
  upr = tmp[,9],
  truth = with(sim_dat$params, c(p, beta, psi))
) %>%
  mutate(
    capture = factor(ifelse(truth <= upr & truth >= lwr, 1, 0))
  )

tbl0 %>%
  filter(!grepl("psi", param)) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()

tbl0 %>%
  filter(grepl("psi", param)) %>%
  sample_n(50) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()
```

#### model1

```{r, eval = T}
# load fitted model from Julia
fit_parallel <- readJMCMC("julia code/idyllic/model1.rds")
tmp <- nimble_summary(fit_parallel, warmup = 0)
tmp[1:11,c(1,5,9:11)] 

# prediction
tbl1 <- tibble(
  param = rownames(tmp),
  mean = tmp[,1],
  lwr = tmp[,5],
  upr = tmp[,9],
  truth = with(sim_dat$params, c(p, tau, beta, theta, psi))
) %>%
  mutate(
    capture = factor(ifelse(truth <= upr & truth >= lwr, 1, 0))
  )

tbl1 %>%
  filter(!grepl("psi", param)) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()

tbl1 %>%
  filter(grepl("psi", param)) %>%
  sample_n(50) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()
```

#### Compare

```{r}
bind_rows(
  tbl0 %>% mutate(model = "model0"),
  tbl1 %>% mutate(model = "model1")
) %>%
  filter(!grepl("psi", param)) %>%
  filter(!grepl("theta", param)) %>%
  filter(!grepl("tau", param)) %>%
  mutate(
    dif = mean - truth,
    lwr_dif = lwr - truth,
    upr_dif = upr - truth
  ) %>%
  ggplot() +
  geom_pointrange(
    aes(
      xmin = lwr_dif, x = dif, xmax = upr_dif,
      y = param, color = model
    ), position = position_dodge2(width = 1)
  ) +
  theme_bw() +
  geom_vline(aes(xintercept = 0))

bind_rows(
  tbl0 %>% mutate(model = "model0"),
  tbl1 %>% mutate(model = "model1")
) %>%
  filter(
    param %in% paste0("psi[", sample(1:n(), 50), "]")
  ) %>%
  mutate(
    dif = mean - truth,
    lwr_dif = lwr - truth,
    upr_dif = upr - truth
  ) %>%
  ggplot() +
  geom_pointrange(
    aes(
      xmin = lwr_dif, x = dif, xmax = upr_dif,
      y = param, color = model
    ), position = position_dodge2(width = 1)
  ) +
  theme_bw() +
  geom_vline(aes(xintercept = 0))
```

## Missing covariate information

Another way to imagine the data generating mechanism is as a consequence of missing covariate information. Suppose that the true data generating model is
\[
\begin{split}
Z_i &\sim \text{Bernoulli}(\psi_i) \\
y_i &\sim \text{Bernoulli}(z_i p_{ij})
\end{split}
\]
where $\text{logit}(\psi_i) = \beta0 + x_{1,i}'\beta_1 + x_{2,i}'\beta_2$. Further suppose that we only directly observe $x_1$. In this case, the role of the experts is to adjust our mispecified model by informing hyper-priors on random effects that can be used as a surrogate for the missing covariance information. During data generation, this is achieved by thresholding the true probability surface to match the Daubenmire classes.

### Simulated data

```{r}
# functions to simulate data
fitnorm <- function(lwr, upr, delta = 1e-3, root = 2){
  # fit normal distribution such that
  ## mu = logit(midpt)
  ## Pr(Y < logit(lwr)) = .025
  logit <- function(p) log(p / (1-p))
  
  if(lwr == 0) lwr <- lwr + delta
  if(upr == 1) upr <- upr - delta
  
  # precalc quantities
  midpt <- (lwr + upr)/2
  mu <- logit(midpt)
  
  f <- function(gamma){
    pnorm(logit(upr), mu, abs(mu)^(1/root) * exp(gamma)) - pnorm(logit(lwr), mu, abs(mu)^(1/root) * exp(gamma)) - .95
  }
  root <- uniroot(f, c(-10, 10))
  return(c(mu, exp(root$root)))
}
split_poly <- function(grid, n_areas){
  # combine
  sf_poly = grid %>% st_as_sf %>% st_union 
  
  # create random points
  points_rnd <- st_sample(sf_poly, size = 10000)
  
  # k-means clustering
  points <- do.call(rbind, st_geometry(points_rnd)) %>%
    as_tibble() %>%
    setNames(c("lon","lat"))
  k_means <- kmeans(points, centers = n_areas)
  
  # create voronoi polygons
  voronoi_polys <- st_voronoi(
    st_as_sf(
      k_means$centers %>% 
        as_tibble, 
      coords = c("lon", "lat")
    ) %>% st_union,
    sf_poly
  )
  
  # convert to multipolygons
  tmp <- st_cast(voronoi_polys) %>%
    st_as_sf()
  
  group <- matrix(NA, nrow(grid), 1)
  coverby <- st_covered_by(grid %>% st_as_sf, tmp %>% st_buffer(.5))
  for(i in 1:nrow(grid)){
    if(length(coverby[[i]]) == 0){
      NULL
    } else{
      group[i] <- coverby[[i]]  
    }
  }
  out <- grid
  out$group <- c(group)
  
  out %>%
    st_as_sf %>% 
    ggplot() + 
    geom_sf(aes(fill = group))
  return(out)
}

# n = 100
# J = 8
# seed = 1
# beta = c(0, -1, 1)
# p = .5
# tau = 1
# root = 3

sim_missingcov <- function(n = 100, J = 8, seed = 1, beta = c(0, -1, 1), p = .5, tau = 1, root = 3){
  # housekeeping
  if((sqrt(n) %% 1) != 0) stop("n should be a perfect square")
  set.seed(seed)
  sigma = 1
  phi = 3
  
  # bring along the daub classes
  logit <- function(p) log(p / (1-p))
  daub <- round(tibble(
    `Class` = 1:6,
    `Lower` = c(0, .05, .25, .50, .75, .95),
    `Upper` = c(.05, .25, .50, .75, .95, 1)
  ) %>%
    mutate(`Midpoint` = (Lower + Upper)/2) %>%
    mutate(
      logitL = logit(Lower+.01), 
      logitM = logit(Midpoint),
      logitU = logit(Upper-.01)
    ), 2)
  
  distr_tbl <- t(apply(
    daub %>% dplyr::select(Lower, Upper) %>% as.matrix,
    1, 
    function(x) fitnorm(x[1], x[2], delta = 1e-3, root = root)
  )) %>% unname
  
  # generate the data
  # create grid
  sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(sqrt(n),0), c(sqrt(n),sqrt(n)), c(0,0)))))
  grid <- st_as_sf(st_make_grid(sfc, cellsize = 1, square = TRUE)) %>% as_tibble
  names(grid) <- "geometry"
  rm(sfc)
  
  # spatial random effects
  coords <- suppressWarnings({grid %>%
      st_as_sf %>%
      st_centroid %>%
      st_coordinates})
  dist_mat <- coords %>%
    as.matrix %>%
    dist %>% as.matrix
  
  # generate spatial covariate
  Sigma <- sigma^2 * exp(-dist_mat^2 / (2*phi^2)) + diag(1e-1, dim(dist_mat))
  grid$x <- c(mvtnorm::rmvnorm(1, rep(0, n), Sigma))
  grid$x2 <- c(mvtnorm::rmvnorm(1, rep(0, n), Sigma))

  # generate z states
  grid$linpred <- beta[1] + beta[2]*grid$x + beta[3]*grid$x2
  grid$psi <- exp(grid$linpred) / (1 + exp(grid$linpred))
  grid$z <- rbinom(nrow(grid), 1, grid$psi)

  # add group by thresholding true prob according to daubenmire
  grid$group <- sapply(grid$psi, function(x){which((x >= daub$Lower) & (x <= daub$Upper))})
    
  # generate observed detections
  grid$y <- rbinom(nrow(grid), J, grid$z * p)
  out <- list(
    df = grid,
    params = list(
      beta = beta,
      p = p, psi = grid$psi, 
      z = grid$z
    ), 
    distr_tbl = distr_tbl
  )
  
  return(out)
}

# sim data
sim_dat <- sim_missingcov(n = 20^2, seed = 2, beta = c(0, -1, 1))

# plot
p1 <- sim_dat$df %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = x)) +
  theme_bw()
p2 <- sim_dat$df %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = x2)) +
  theme_bw()
p3 <- sim_dat$df %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = psi)) +
  theme_bw()
p4 <- sim_dat$df %>%
  mutate(z = factor(z)) %>%
  st_as_sf() %>%
  ggplot() + 
  geom_sf(aes(fill = z)) +
  theme_bw()

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

### Fit models with Julia

We consider two potential models:

  - model0: basic occupancy model without region-specific random effects
  - model1: occupancy model with region-specific random effects informed by experts

For the time being, we assume the experts are correct.

```{r, eval = F}
# export data list to Julia
getC <- function(dat){
  C <- matrix(0, nrow(dat$df), 6)
  for(i in 1:nrow(C)){
    C[i, dat$df$group[i]] <- 1
  }
  sums <- colSums(C)
  
  if(any(sums == 0)){
    keep <- which(sums != 0)
    out = list(
      C = C[,keep], 
      mean_vec = dat$distr_tbl[keep,1],
      sd_vec = dat$distr_tbl[keep,2]
    )
  } else{
    out = list(
      C = C, 
      mean_vec = dat$distr_tbl[,1],
      sd_vec = dat$distr_tbl[,2]
    )
  }
  
  return(out)
}

data_list <- list(
  y = sim_dat$df$y,
  J = 8,
  X = cbind(rep(1, nrow(sim_dat$df)), sim_dat$df$x),
  C = getC(sim_dat)$C,
  expert_means = getC(sim_dat)$mean_vec,
  expert_sds = getC(sim_dat)$sd_vec
)
saveRDS(data_list, "julia code/missingcovs/data_list.rds")
```

#### model0

```{r, eval = T}
# load fitted model from Julia
fit_parallel <- readJMCMC("julia code/missingcovs/model0.rds")
tmp <- nimble_summary(fit_parallel, warmup = 0)
tmp[1:11,c(1,5,9:11)] 

# prediction
tbl0 <- tibble(
  param = rownames(tmp),
  mean = tmp[,1],
  lwr = tmp[,5],
  upr = tmp[,9],
  truth = with(sim_dat$params, c(p, beta[1:2], psi))
) %>%
  mutate(
    capture = factor(ifelse(truth <= upr & truth >= lwr, 1, 0))
  )

tbl0 %>%
  filter(!grepl("psi", param)) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()

tbl0 %>%
  filter(grepl("psi", param)) %>%
  sample_n(50) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()
```

#### model1

```{r, eval = T}
# load fitted model from Julia
fit_parallel <- readJMCMC("julia code/missingcovs/model1.rds")
tmp <- nimble_summary(fit_parallel, warmup = 0)
tmp[1:11,c(1,5,9:11)] 

# prediction
tbl1 <- tibble(
  param = rownames(tmp),
  mean = tmp[,1],
  lwr = tmp[,5],
  upr = tmp[,9],
  truth = with(sim_dat$params, c(p, NA, beta[1:2], rep(NA, 5), psi))
) %>%
  mutate(
    capture = factor(ifelse(truth <= upr & truth >= lwr, 1, 0))
  )

tbl1 %>%
  filter(!grepl("psi", param)) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()

tbl1 %>%
  filter(grepl("psi", param)) %>%
  sample_n(50) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()
```

#### Compare

```{r}
bind_rows(
  tbl0 %>% mutate(model = "model0"),
  tbl1 %>% mutate(model = "model1")
) %>%
  filter(!grepl("psi", param)) %>%
  filter(!grepl("theta", param)) %>%
  filter(!grepl("tau", param)) %>%
  mutate(
    dif = mean - truth,
    lwr_dif = lwr - truth,
    upr_dif = upr - truth
  ) %>%
  ggplot() +
  geom_pointrange(
    aes(
      xmin = lwr_dif, x = dif, xmax = upr_dif,
      y = param, color = model
    ), position = position_dodge2(width = 1)
  ) +
  theme_bw() +
  geom_vline(aes(xintercept = 0))

bind_rows(
  tbl0 %>% mutate(model = "model0"),
  tbl1 %>% mutate(model = "model1")
) %>%
  left_join(
    ., 
    tibble(
      param = paste0("psi[", 1:nrow(sim_dat$df), "]"),
      group = sim_dat$df$group
    )
  ) %>%
  mutate(group = factor(group)) %>%
  filter(
    param %in% paste0("psi[", sample(1:(n()/2), 20), "]")
  ) %>%
  mutate(
    dif = mean - truth,
    lwr_dif = lwr - truth,
    upr_dif = upr - truth
  ) %>%
  arrange(param) %>%
  ggplot() +
  geom_pointrange(
    aes(
      xmin = lwr_dif, x = dif, xmax = upr_dif,
      y = param, color = model
    ), position = position_dodge2(width = 1)
  ) +
  theme_bw() +
  geom_vline(aes(xintercept = 0)) 

bind_rows(
  tbl0 %>% mutate(model = "model0"),
  tbl1 %>% mutate(model = "model1")
) %>%
  left_join(
    ., 
    tibble(
      param = paste0("psi[", 1:nrow(sim_dat$df), "]"),
      group = sim_dat$df$group
    )
  ) %>%
  mutate(group = factor(group)) %>%
  filter(
    param %in% paste0("psi[", sample(1:(n()/2), 20), "]")
  ) %>%
  mutate(
    dif = mean - truth,
    lwr_dif = lwr - truth,
    upr_dif = upr - truth
  ) %>%
  arrange(param) %>%
  ggplot() +
  geom_pointrange(
    aes(
      xmin = lwr_dif, x = dif, xmax = upr_dif,
      y = param, color = model
    ), position = position_dodge2(width = 1)
  ) +
  theme_bw() +
  geom_vline(aes(xintercept = 0)) +
  facet_wrap(~ group)
```





# Appendix: Models in Stan

Below we provide `stan` code to fit the above model and showcase it on the data from the idyllic scenario. This model takes approximately 30 seconds to draw 5000 posterior samples (opposed to ~ 3 seconds for the Julia version). 

```{r}
sim_dat <- sim_idyllic(n = 20^2, seed = 2, nregions = 6, beta = c(0, 1))
```

```{r}
m <- stan_model("julia code/idyllic/stan_occ.stan")
m
```

```{r, eval = F}
fit <- sampling(
  m,
  data = list(
    N = length(sim_dat$df$y),
    J = 8,
    sumy = sim_dat$df$y,
    x = sim_dat$df$x,
    K = 7,
    group = sim_dat$df$group,
    m = sim_dat$expert_dists[,1],
    s = sim_dat$expert_dists[,2]
  ),
  iter = 5000,
  chains = 3,
  pars = c("tau", "p", "beta0", "beta1", "theta", "psi")
)
saveRDS(fit, file = "julia code/idyllic/stan_fit.rds")
```

```{r}
stan_fit <- readRDS("julia code/idyllic/stan_fit.rds")
summary(stan_fit)$summary[1:11,c(1, 4, 8:10)]
tmp <- summary(stan_fit)$summary[,c(1, 4, 8:10)]
tmp <- tmp[-nrow(tmp),]

# plot
tbl1 <- tibble(
  param = rownames(tmp),
  mean = tmp[,1],
  lwr = tmp[,2],
  upr = tmp[,3],
  truth = with(sim_dat$params, c(tau, p, beta, theta, psi))
) %>%
  mutate(
    capture = factor(ifelse(truth <= upr & truth >= lwr, 1, 0))
  )

tbl1 %>%
  filter(!grepl("psi[[]", param)) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()

tbl1 %>%
  filter(grepl("psi[[]", param)) %>%
  sample_n(50) %>%
  ggplot() +
  geom_pointrange(aes(xmin = lwr, x = mean, xmax = upr, y = param, color = capture)) +
  geom_point(aes(x = truth, y = param)) +
  theme_bw()
```

